{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project: Temporal-Difference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Explore CliffWalkingEnv\n",
    "\n",
    "Use the code cell below to create an instance of the [CliffWalking](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py) environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves through a $4\\times 12$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    " [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    " [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    " [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "```\n",
    "At the start of any episode, state `36` is the initial state.  State `47` is the only terminal state, and the cliff corresponds to states `37` through `46`.\n",
    "\n",
    "The agent has 4 potential actions:\n",
    "```\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+=\\{0, 1, \\ldots, 47\\}$, and $\\mathcal{A} =\\{0, 1, 2, 3\\}$.  Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-project, we will build towards finding the optimal policy for the CliffWalking environment.  The optimal state-value function is visualized below.  Please take the time now to make sure that you understand _why_ this is the optimal state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plot_utils import plot_values\n",
    "\n",
    "# define the optimal state-value function\n",
    "V_opt = np.zeros((4,12))\n",
    "V_opt[0] = -np.arange(3, 15)[::-1]\n",
    "V_opt[1] = -np.arange(3, 15)[::-1] + 1\n",
    "V_opt[2] = -np.arange(3, 15)[::-1] + 2\n",
    "V_opt[3][0] = -13\n",
    "\n",
    "plot_values(V_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: TD Prediction: State Values\n",
    "\n",
    "In this section, you will write your own implementation of TD prediction (for estimating the state-value function).\n",
    "\n",
    "We will begin by investigating a policy where the agent moves:\n",
    "- `RIGHT` in states `0` through `10`, inclusive,  \n",
    "- `DOWN` in states `11`, `23`, and `35`, and\n",
    "- `UP` in states `12` through `22`, inclusive, states `24` through `34`, inclusive, and state `36`.\n",
    "\n",
    "The policy is specified and printed below.  Note that states where the agent does not choose an action have been marked with `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.hstack([1*np.ones(11), 2, 0, np.zeros(10), 2, 0, np.zeros(10), 2, 0, -1*np.ones(11)])\n",
    "print(\"\\nPolicy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy.reshape(4,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the state-value function that corresponds to this policy.  Make sure that you take the time to understand why this is the corresponding value function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_true = np.zeros((4,12))\n",
    "for i in range(3):\n",
    "    V_true[0:12][i] = -np.arange(3, 15)[::-1] - i\n",
    "V_true[1][11] = -2\n",
    "V_true[2][11] = -1\n",
    "V_true[3][0] = -17\n",
    "\n",
    "plot_values(V_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is what you will try to approximate through the TD prediction algorithm.\n",
    "\n",
    "Your algorithm for TD prediction has five arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `policy`: This is a 1D numpy array with `policy.shape` equal to the number of states (`env.nS`).  `policy[s]` returns the action that the agent chooses when in state `s`.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `V`: This is a dictionary where `V[s]` is the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import sys\n",
    "\n",
    "def td_prediction(env, num_episodes, policy, alpha, gamma=1.0):\n",
    "    # initialize empty dictionaries of floats\n",
    "    V = defaultdict(float)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = policy[state]\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            V[state] += alpha*(reward + gamma * V[next_state] - V[state])\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "    return V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your implementation and visualize the estimated state-value function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_test\n",
    "\n",
    "# evaluate the policy and reshape the state-value function\n",
    "V_pred = td_prediction(env, 5000, policy, .01)\n",
    "\n",
    "# please do not change the code below this line\n",
    "V_pred_plot = np.reshape([V_pred[key] if key in V_pred else 0 for key in np.arange(48)], (4,12)) \n",
    "check_test.run_check('td_prediction_check', V_pred_plot)\n",
    "plot_values(V_pred_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close is your estimated state-value function to the true state-value function corresponding to the policy?  \n",
    "\n",
    "You might notice that some of the state values are not estimated by the agent.  This is because under this policy, the agent will not visit all of the states.  In the TD prediction algorithm, the agent can only estimate the values corresponding to states that are visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: TD Control: Sarsa\n",
    "\n",
    "In this section, you will write your own implementation of the Sarsa control algorithm.\n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     20
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def generate_micro_policy(q_s, e, n_actions):\n",
    "    if np.array_equal(q_s, np.zeros(n_actions)):\n",
    "        return np.ones(n_actions) * 1/n_actions  \n",
    "    \n",
    "    micro_policy_s = np.ones(n_actions) * [e/n_actions]\n",
    "    best_action = np.argmax(q_s)\n",
    "    micro_policy_s[best_action] = 1 - e + e/n_actions\n",
    "    \n",
    "    return micro_policy_s\n",
    "    \n",
    "def take_an_action(Q, state, e, env):\n",
    "    n_actions = env.nA\n",
    "    policy = generate_micro_policy(Q[state], e, n_actions)\n",
    "    \n",
    "    return np.random.choice(np.arange(n_actions),\n",
    "                            p=policy)\n",
    "\n",
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    # loop over episodes\n",
    "    \n",
    "    scores = []\n",
    "    actions = []\n",
    "    \n",
    "    plot_every = 25\n",
    "    \n",
    "    deque_scores = deque(maxlen=plot_every)\n",
    "    deque_actions = deque(maxlen=plot_every)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()   \n",
    "        \n",
    "        e = 1.0/i_episode\n",
    "        \n",
    "        state = env.reset()\n",
    "        action = take_an_action(Q, state, e, env)\n",
    "        \n",
    "        #create an score for further analysis\n",
    "        score_in_episode = 0\n",
    "        \n",
    "        #max od 300 actions per episode\n",
    "        for actions_made in range(300):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_action = take_an_action(Q, next_state, e, env)\n",
    "            \n",
    "            Q[state][action] += alpha*(reward\n",
    "                                       + gamma * Q[next_state][next_action]\n",
    "                                       - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            score_in_episode += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        #append score for further plot\n",
    "        deque_scores.append(score_in_episode)\n",
    "        deque_actions.append(actions_made)\n",
    "        \n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(deque_scores))\n",
    "            actions.append(np.mean(deque_actions))\n",
    "                \n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0, num_episodes, len(scores), endpoint=False),\n",
    "             np.asarray(scores))\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(np.linspace(0, num_episodes, len(actions), endpoint=False),\n",
    "             np.asarray(actions))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Actions made (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the **_estimated_** optimal policy and the corresponding state-value function.  \n",
    "\n",
    "If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_sarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "plot_values(V_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: TD Control: Q-learning\n",
    "\n",
    "In this section, you will write your own implementation of the Q-learning control algorithm.\n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#q-learning\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    \n",
    "    scores = []\n",
    "    actions = []\n",
    "    \n",
    "    plot_every = 25\n",
    "    \n",
    "    deque_scores = deque(maxlen=plot_every)\n",
    "    deque_actions = deque(maxlen=plot_every)\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function \n",
    "        \n",
    "        e = 1/i_episode\n",
    "        \n",
    "        state = env.reset()\n",
    "        action = take_an_action(Q, state, e, env)\n",
    "        \n",
    "        #create an score for further analysis\n",
    "        score_in_episode = 0\n",
    "        actions_made = 0\n",
    "        \n",
    "        #max od 300 actions per episode\n",
    "        for actions_made in range(300):\n",
    "            action = take_an_action(Q, state, e, env)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            Q[state][action] += alpha*(reward \n",
    "                                       + gamma * np.max(Q[next_state])\n",
    "                                       - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            score_in_episode += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        #append score for further plot\n",
    "        deque_scores.append(score_in_episode)\n",
    "        deque_actions.append(actions_made)\n",
    "        \n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(deque_scores))\n",
    "            actions.append(np.mean(deque_actions))\n",
    "                \n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0, num_episodes, len(scores), endpoint=False),\n",
    "             np.asarray(scores))\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(np.linspace(0, num_episodes, len(actions), endpoint=False),\n",
    "             np.asarray(actions))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Actions made (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the **_estimated_** optimal policy and the corresponding state-value function. \n",
    "\n",
    "If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsamax = q_learning(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsamax = np.array([np.argmax(Q_sarsamax[key]) if key in Q_sarsamax else -1 for key in np.arange(48)]).reshape((4,12))\n",
    "check_test.run_check('td_control_check', policy_sarsamax)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsamax)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_sarsamax[key]) if key in Q_sarsamax else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: TD Control: Expected Sarsa\n",
    "\n",
    "In this section, you will write your own implementation of the Expected Sarsa control algorithm.\n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    \n",
    "    scores = []\n",
    "    actions = []\n",
    "    \n",
    "    plot_every = 25\n",
    "    \n",
    "    deque_scores = deque(maxlen=plot_every)\n",
    "    deque_actions = deque(maxlen=plot_every)\n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        ## s very small epsilon\n",
    "        e = 1 / (num_episodes * 1000)\n",
    "        \n",
    "        state = env.reset()\n",
    "        action = take_an_action(Q, state, e, env)\n",
    "        \n",
    "        #create an score for further analysis\n",
    "        score_in_episode = 0\n",
    "        actions_made = 0\n",
    "        \n",
    "        #max od 300 actions per episode\n",
    "        for actions_made in range(300):\n",
    "            action = take_an_action(Q, state, e, env)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            policy = generate_micro_policy(Q[next_state], e, env.nA)\n",
    "            \n",
    "            Q[state][action] += alpha*(reward \n",
    "                                       + gamma * policy.dot(Q[next_state])\n",
    "                                       - Q[state][action])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            score_in_episode += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        #append score for further plot\n",
    "        deque_scores.append(score_in_episode)\n",
    "        deque_actions.append(actions_made)\n",
    "        \n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(deque_scores))\n",
    "            actions.append(np.mean(deque_actions))\n",
    "                \n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0, num_episodes, len(scores), endpoint=False),\n",
    "             np.asarray(scores))\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(np.linspace(0, num_episodes, len(actions), endpoint=False),\n",
    "             np.asarray(actions))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Actions made (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the **_estimated_** optimal policy and the corresponding state-value function.  \n",
    "\n",
    "If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_expsarsa = expected_sarsa(env, 5000, 1)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_expsarsa = np.array([np.argmax(Q_expsarsa[key]) if key in Q_expsarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_expsarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_expsarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_expsarsa[key]) if key in Q_expsarsa else 0 for key in np.arange(48)])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
