{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The pendulum challenge is to keep a \n",
    "frictionless pendulum standing up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "# The Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "| Num | Observation |\n",
    "|:-:|:-:|\n",
    "| 0 | cos(theta) |\n",
    "| 1 | sin(theta) |\n",
    "| 2 | theta dot |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<style>\n",
    "td {\n",
    "  font-size: 100px\n",
    "}\n",
    "    \n",
    "    \n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the state space: (3,)\n",
      "Highest value: [1. 1. 8.]\n",
      "Lowest value: [-1. -1. -8.]\n",
      "A sample state: [0.81256366 0.45644432 1.6889504 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the state space: {env.observation_space.shape}\")\n",
    "print(f\"Highest value: {env.observation_space.high}\")\n",
    "print(f\"Lowest value: {env.observation_space.low}\")\n",
    "\n",
    "env.observation_space\n",
    "\n",
    "print(f\"A sample state: {env.observation_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "| Num | Action |\n",
    "|:-:|:-:|\n",
    "| 0 | Joint effort |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the action space: (1,)\n",
      "Highest value: [2.]\n",
      "Lowest value: [-2.]\n",
      "A sample action: [-0.8608973]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the action space: {env.action_space.shape}\")\n",
    "print(f\"Highest value: {env.action_space.high}\")\n",
    "print(f\"Lowest value: {env.action_space.low}\")\n",
    "\n",
    "env.action_space.seed(473)\n",
    "\n",
    "print(f\"A sample action: {env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8785995], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Function: $-(theta^2 + 0.1*thetaDot^2 + 0.001*action^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "class ScoreEvaluator:\n",
    "    def __init__(self, window: int):\n",
    "        self.window = window\n",
    "        self.best_score = -np.inf\n",
    "        self.avg_scores = []\n",
    "        self.tmp_scores = deque(maxlen=window)\n",
    "        self.last_score = None\n",
    "        \n",
    "    def add(self, score: float):\n",
    "        if not score:\n",
    "            raise ValueError(f'Score could not be {score}')\n",
    "         \n",
    "        self.tmp_scores.append(score)\n",
    "        \n",
    "        if score > self.best_score:\n",
    "            self.best_score = score\n",
    "        \n",
    "        self._update_avg()\n",
    "        self.last_score = score\n",
    "        \n",
    "    def plot_avg_scores(self):\n",
    "        plt.plot(np.linspace(0,\n",
    "                             len(self.avg_scores),\n",
    "                             len(self.avg_scores),\n",
    "                             endpoint=False),\n",
    "                 np.asarray(self.avg_scores))\n",
    "        plt.title(f'Best Reward: {self.best_score:10.5f}')\n",
    "        plt.xlabel('Episode Number')\n",
    "        plt.ylabel(f'Average Actions made (Over Next {self.window} Episodes)')\n",
    "        rolling_mean = (pd.Series(self.avg_scores)\n",
    "                          .rolling(199)\n",
    "                          .mean())\n",
    "        plt.plot(rolling_mean);\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def _update_avg(self):\n",
    "        if len(self.tmp_scores) < self.tmp_scores.maxlen:\n",
    "            return\n",
    "        self.avg_scores.append(np.mean(self.tmp_scores))\n",
    "        \n",
    "def print_iteaction(iteraction: int, score_eval: ScoreEvaluator):\n",
    "    \"function responsible to print some infos each iteration\"\n",
    "    print(\"{:4d} - Best Score: {:5.2f} - {:5.2f}\".format(iteraction,\n",
    "                                                         score_eval.best_score,\n",
    "                                                         score_eval.last_score),\n",
    "          end=\"\\r\",\n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Dummy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class DummyAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        return [np.random.normal(0., .8)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Policy-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "class PolicyAgent():\n",
    "    def __init__(self, env):\n",
    "        # Task (environment) information\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        self.w = np.random.normal(\n",
    "            size=(self.state_size, self.action_size),\n",
    "            scale=(self.action_range / 10))\n",
    "\n",
    "        self.best_w1 = None\n",
    "        \n",
    "        self.best_score = -np.inf\n",
    "        \n",
    "        self.noise = 1\n",
    "        \n",
    "        self.alpha = 1e-10\n",
    "\n",
    "        self.reset_episode()\n",
    "        \n",
    "        self.find_better = 1\n",
    "        \n",
    "        self.std = self.action_range / 1000\n",
    "        \n",
    "        \n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, next_state, reward, done, action):\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.learn(action, reward)\n",
    "#         if done:\n",
    "#             self.learn(action)\n",
    "\n",
    "    def act(self, state):\n",
    "        mean = np.dot(state, self.w)\n",
    "\n",
    "        self.last_state = state\n",
    "\n",
    "        return np.random.normal(mean, scale=self.std)\n",
    "    \n",
    "    def learn(self, action, reward):\n",
    "        grad = self._gradiant(action, np.array([self.last_state]))\n",
    "        delta = self.alpha * grad * reward\n",
    "        self.w += delta.T\n",
    "    \n",
    "    def learn_in_the_end(self, action):\n",
    "        grad = self._gradiant(action, np.array([self.last_state]))\n",
    "        delta = self.alpha * grad * self.total_reward\n",
    "\n",
    "        self.w -= delta.T\n",
    "        \n",
    "    def _gradiant(self, action, state):\n",
    "        # -1 / (5*stdÂ³)\n",
    "        cal = (-1 / (5 * self.std ** 3)) * (2*action*state - 2*state)\n",
    "        return cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from uuid import uuid4\n",
    "\n",
    "class Buffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size: maximum size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",\n",
    "                                     field_names=[\"session\",\n",
    "                                                  \"id\",\n",
    "                                                  \"state\",\n",
    "                                                  \"action\",\n",
    "                                                  \"reward\",\n",
    "                                                  \"next_state\",\n",
    "                                                  \"done\"])\n",
    "\n",
    "    def add(self, session, id, state, action, reward,\n",
    "            next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(session,\n",
    "                            id,\n",
    "                            state,\n",
    "                            action,\n",
    "                            reward,\n",
    "                            next_state,\n",
    "                            done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def change_reward(self, position, reward):\n",
    "        exp_temp = list(self.memory[position])\n",
    "        exp_temp[4] = reward\n",
    "        self.memory[position] = self.experience(*exp_temp)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        return random.sample(self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "# testing change_reward\n",
    "x = Buffer(2,2)\n",
    "x.add(1,2,3,4,5,6,7)\n",
    "x.change_reward(0, 377)\n",
    "assert x.memory[0].reward == 377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Reinforce Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class ReinforceAgent():\n",
    "    def __init__(self, env):\n",
    "        # Task (environment) information\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        self.w = np.random.normal(\n",
    "            size=(self.state_size, self.action_size),\n",
    "            scale=(self.action_range) / 10)\n",
    "        \n",
    "        self.w2 = np.random.normal(\n",
    "            size=(self.state_size, self.action_size),\n",
    "            scale=(self.action_range) / 10)\n",
    "        \n",
    "        self.alpha = 1e-10\n",
    "   \n",
    "        self.find_better = 1\n",
    "        \n",
    "        self.std = self.action_range / 10\n",
    "        \n",
    "        self.buffer = Buffer(512, 32)\n",
    "\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.cumulative_reward = 0.0\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def act(self, state):\n",
    "        mean = self._mean(state)\n",
    "        std = self._std(state)\n",
    "        action = np.random.normal(mean, scale=std)\n",
    "    \n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        return mean\n",
    "    \n",
    "    \n",
    "    def step(self, next_state, reward, done):        \n",
    "        self.buffer.add(self.last_state,\n",
    "                        self.last_action,\n",
    "                        reward,\n",
    "                        next_state,\n",
    "                        done,\n",
    "                        self.cumulative_reward)\n",
    "        \n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        if done:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        for (state, action, reward, next_state,\n",
    "             done, cumulative_reward) in self.buffer.memory:\n",
    "            grad = self._gradiant(state, action).T\n",
    "            \n",
    "            self.w += (self.alpha\n",
    "                       * grad\n",
    "                       * reward)\n",
    "            \n",
    "        for (state, action, reward, next_state,\n",
    "             done, cumulative_reward) in self.buffer.sample():\n",
    "            grad = self._gradiant(state, action).T          \n",
    "\n",
    "            self.w += (self.alpha\n",
    "                       * grad\n",
    "                       * (self.cumulative_reward\n",
    "                          - cumulative_reward))   \n",
    "        \n",
    "    def _mean(self, s):\n",
    "        return np.dot(s, self.w)\n",
    "    \n",
    "    def _std(self, s):\n",
    "        return np.dot(s, self.w2)\n",
    "    \n",
    "    def _gradiant(self, s, action):\n",
    "        mean = self._mean(s)       \n",
    "        \n",
    "        cal = (s / (self.std ** 2)) * (action - mean)\n",
    "        return np.array([cal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Reinforce Algorithm std and Mean variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class ReinforceAgentAdvanced():\n",
    "    def __init__(self, env):\n",
    "        # Task (environment) information\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape[0] * 3\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        self.w = np.random.normal(\n",
    "            size=(self.state_size, self.action_size),\n",
    "            scale=(self.action_range) / 4)\n",
    "        \n",
    "        self.w2 = abs(np.random.normal(\n",
    "            size=(self.state_size, self.action_size),\n",
    "            scale=(self.action_range) / 4))\n",
    "        \n",
    "        self.alpha = 1e-1\n",
    "        self.gamma = 1\n",
    "   \n",
    "        self.find_better = 1\n",
    "        \n",
    "        self.all_rewards = []\n",
    "        \n",
    "        self.buffer = Buffer(30000, 192)\n",
    "        self.sample_size = 512\n",
    "\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "#         self.buffer.clear()\n",
    "        self.all_rewards.clear()\n",
    "        self.current_id = 0\n",
    "        self.session = str(uuid4().hex)\n",
    "\n",
    "    def act(self, state, explore=False):\n",
    "        if explore:\n",
    "            action = np.array([np.random.uniform(-1,1)])\n",
    "        else:\n",
    "            mean = self._mean(state)\n",
    "            std = self._std(state)\n",
    "            action = np.random.normal(mean, scale=std)\n",
    "    \n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def step(self, next_state, reward, done):\n",
    "        self.buffer.add(self.session,\n",
    "                        self.current_id,\n",
    "                        self.last_state,\n",
    "                        self.last_action,\n",
    "                        reward,\n",
    "                        next_state,\n",
    "                        done)\n",
    "        \n",
    "        self.current_id += 1\n",
    "        self.all_rewards.append(reward)\n",
    "\n",
    "        self.learn(done)\n",
    "            \n",
    "    def learn(self, done):        \n",
    "        mean_discount_reward = np.mean(self.all_rewards)\n",
    "        std_discount_reward = np.std(self.all_rewards)\n",
    "        \n",
    "        # update all rewards in buffer \n",
    "        # with cumulative normalized reward\n",
    "        if done:\n",
    "            for position, (session, id, _, _, reward, _ ,\n",
    "                           _) in enumerate(self.buffer.memory):          \n",
    "                # avoiding rows for different sessions (episodes)\n",
    "                if session != self.session:\n",
    "                    continue\n",
    "\n",
    "                # creating generator with gammas\n",
    "                dis = (self.gamma ** i for i in count(start=0, step=1))\n",
    "\n",
    "                # using the gen and the rewards to calc cumulative\n",
    "                cumulative_reward = sum([r*d for r, d\n",
    "                                         in zip(self.all_rewards[id:],\n",
    "                                                dis)])\n",
    "\n",
    "                # normalizing the cumulative reward\n",
    "                normal_rew = ((cumulative_reward - mean_discount_reward)\n",
    "                              / std_discount_reward)\n",
    "\n",
    "                # changing the reward on the buffer (using raw position)\n",
    "                self.buffer.change_reward(position, normal_rew) \n",
    "        \n",
    "        for (session, id, state, action, reward, next_state,\n",
    "             done) in self.buffer.sample(min(len(self.buffer.memory),\n",
    "                                             self.sample_size)):\n",
    "            if session != self.session:\n",
    "                continue \n",
    "                     \n",
    "            grad = self._gradiant(state, action).T \n",
    "            delta = ((1/(id+1))\n",
    "                     * grad\n",
    "                     * reward)\n",
    "            self.w += delta\n",
    "            \n",
    "            grad_2 = self._gradiant_2(state, action).T          \n",
    "            delta_2 = ((1/(id+1))\n",
    "                       * grad_2\n",
    "                       * reward)\n",
    "\n",
    "            self.w2 += delta_2\n",
    "        \n",
    "    def _mean(self, s):\n",
    "        state = np.concatenate((s, [s[0]**2, s[1]**2, s[2]**2,\n",
    "                                  s[0]*s[1], s[0]*s[2], s[1]*s[2]]))\n",
    "        return np.dot(state, self.w)\n",
    "    \n",
    "    def _std(self, s):\n",
    "        state = np.concatenate((s, [s[0]**2, s[1]**2, s[2]**2,\n",
    "                                  s[0]*s[1], s[0]*s[2], s[1]*s[2]]))\n",
    "        return abs(np.dot(state, self.w2))\n",
    "    \n",
    "    def _gradiant(self, s, action):\n",
    "        mean = self._mean(s)       \n",
    "        std = self._std(s)\n",
    "        \n",
    "        state = np.concatenate((s, [s[0]**2, s[1]**2, s[2]**2,\n",
    "                                  s[0]*s[1], s[0]*s[2], s[1]*s[2]]))\n",
    "        \n",
    "        cal = (state / (std ** 2)) * (action - mean)\n",
    "        return np.array([cal])\n",
    "    \n",
    "    def _gradiant_2(self, s, action):\n",
    "        mean = self._mean(s)       \n",
    "        std = self._std(s)\n",
    "        \n",
    "        state = np.concatenate((s, [s[0]**2, s[1]**2, s[2]**2,\n",
    "                                  s[0]*s[1], s[0]*s[2], s[1]*s[2]]))\n",
    "        \n",
    "        cal = state*(mean + std - action)*(-mean + std + action)/(std ** 3)\n",
    "        return np.array([cal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9 - Best Score: -876.23 - -876.2382\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "agent = ReinforceAgentAdvanced(env)\n",
    "\n",
    "best_score = -np.inf\n",
    "\n",
    "avg_scores = deque(maxlen=num_episodes)\n",
    "\n",
    "score_eval = ScoreEvaluator(1)\n",
    "\n",
    "for i in range(num_episodes):  \n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    agent.reset_episode()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.step(state, reward, done)\n",
    "        \n",
    "        score += reward\n",
    "        if done:\n",
    "            score_eval.add(score)\n",
    "            print_iteaction(i, score_eval)\n",
    "            break\n",
    "            \n",
    "score_eval.plot_avg_scores()\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for i in range(2):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        agent.reset_episode()\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### DDPG Actor/Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size: maximum size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        return random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self, env):  \n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        # Actor (Policy) Model\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "\n",
    "        # Critic (Value) Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size)\n",
    "\n",
    "        # Initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        # Noise process\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        self.exploration_sigma = 0.2\n",
    "        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        # Replay memory\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.tau = 0.01  # for soft update of target parameters\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        self.last_state = state\n",
    "\n",
    "    def step(self, action, reward, next_state, done):\n",
    "         # Save experience / reward\n",
    "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "        # Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "        return list(action + self.noise.sample())  # add some noise for exploration\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        #     Q_targets_next = critic_target(next_state, actor_target(next_state))\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "\n",
    "        # Compute Q targets for current states and train critic model (local)\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        # Train actor model (local)\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n",
    "\n",
    "        # Soft-update target models\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)   \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Actor-Critic method simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "from ipdb import set_trace as debug\n",
    "\n",
    "# np.random.seed(2)\n",
    "# tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def gaussian(x):\n",
    "    return K.exp(-K.pow(x,2))\n",
    "\n",
    "class Actor():\n",
    "    def __init__(self, env, lr=0.0001):\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "#         self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "#         self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "#         self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")\n",
    "\n",
    "        states = layers.Input(shape=self.state_size, name='states')\n",
    "        \n",
    "        layer_1 = layers.Dense(\n",
    "            units=30,\n",
    "            activation='relu',\n",
    "            kernel_initializer=keras.initializers.RandomNormal(mean=.0,\n",
    "                                                               stddev=.1),\n",
    "            bias_initializer=keras.initializers.Constant(value=0.1),\n",
    "        )(states)\n",
    "        \n",
    "        median = layers.Dense(\n",
    "            units=1,\n",
    "            activation='tanh',\n",
    "            kernel_initializer=keras.initializers.RandomNormal(mean=.0,\n",
    "                                                               stddev=.1),\n",
    "            bias_initializer=keras.initializers.Constant(value=0.1),\n",
    "        )(layer_1)\n",
    "    \n",
    "        sigma = layers.Dense(\n",
    "            units=1,\n",
    "            activation='softplus',\n",
    "            kernel_initializer=keras.initializers.RandomNormal(mean=.0,\n",
    "                                                               stddev=.1),\n",
    "            bias_initializer=keras.initializers.Constant(value=1.),\n",
    "        )(layer_1)\n",
    "        \n",
    "        combined = layers.concatenate([median, sigma])\n",
    "        \n",
    "        def gaussian(x):\n",
    "            median, sigma = tf.squeeze(x[0][0]*2), tf.squeeze(x[0][1]+0.1)\n",
    "            return K.random_normal((1,), mean=median, stddev=sigma)\n",
    "#             return np.array([np.random.normal(median, abs(sigma))], dtype=float)\n",
    "#             return tf.distributions.Normal(median, sigma)\n",
    "        \n",
    "        actions = layers.Activation(gaussian)(combined)\n",
    "        \n",
    "#         actions = layers.Lambda(gaussian)(combined)\n",
    "        \n",
    "        self.model = models.Model(inputs=[states], outputs=actions)\n",
    "        \n",
    "        # Define optimizer and compile model for training with built-in loss function\n",
    "        optimizer = optimizers.Adam(lr=1e-3)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "#         # Compute action gradients (derivative of Q values w.r.t. to actions)\n",
    "#         action_gradients = K.gradients(Q_values, actions)\n",
    "\n",
    "#         # Define an additional function to fetch action gradients (to be used by actor model)\n",
    "#         self.get_action_gradients = K.function(\n",
    "#             inputs=[*self.model.input, K.learning_phase()],\n",
    "#             outputs=action_gradients)\n",
    "        \n",
    "#         debug()\n",
    "        \n",
    "#         global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "#         self.action = tf.clip_by_value(self.normal_dist.sample(1),\n",
    "#                                        self.action_low,\n",
    "#                                        self.action_high)\n",
    "\n",
    "#         with tf.name_scope('exp_v'):\n",
    "#             log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            \n",
    "#             self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            \n",
    "#             # Add cross entropy cost to encourage exploration\n",
    "#             self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "#         with tf.name_scope('train'):\n",
    "#             self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, state, action, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        return sess.run(self.action, {self.state: s})  # get probabilities for all actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.37464064]], dtype=float32), array([[1.2310231]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = Actor(env)\n",
    "act.model.predict(np.reshape([1,2,3], [-1, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-5-f66113186dff>\u001b[0m(77)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     76 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m        \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.model.predict(np.array([1,2,3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: unexpected EOF while parsing\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.model.predict(np.array([1,2,3]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ValueError: Error when checking input: expected states to have shape (3,) but got array with shape (1,)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-32ebf244065b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f66113186dff>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, lr)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.action = tf.clip_by_value(self.normal_dist.sample(1),\n",
      "\u001b[0;32m<ipython-input-5-f66113186dff>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, lr)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.action = tf.clip_by_value(self.normal_dist.sample(1),\n",
      "\u001b[0;32m/usr/local/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 1000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = -100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(env, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if i_episode > 900:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Actor-Critic method simplified 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "from ipdb import set_trace as debug\n",
    "\n",
    "np.random.seed(37)\n",
    "tf.set_random_seed(43)  # reproducible\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "class Actor():\n",
    "    def __init__(self, env, lr=0.0001):\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape\n",
    "        self.action_size = env.action_space.shape\n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.adam_optimizer = self.optimizer()\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        states = layers.Input(shape=self.state_size, name='states')\n",
    "        \n",
    "        layer_1 = layers.Dense(\n",
    "            units=30,\n",
    "            activation='relu',\n",
    "            kernel_initializer=keras.initializers.RandomNormal(mean=.0,\n",
    "                                                               stddev=.1),\n",
    "            bias_initializer=keras.initializers.Constant(value=0.1),\n",
    "        )(states)\n",
    "        \n",
    "        median = layers.Dense(\n",
    "            units=1,\n",
    "            activation='tanh',\n",
    "            kernel_initializer=keras.initializers.RandomNormal(mean=.0,\n",
    "                                                               stddev=.1),\n",
    "            bias_initializer=keras.initializers.Constant(value=0.1),\n",
    "        )(layer_1)\n",
    "        \n",
    "        output = layers.Lambda(lambda i: i * self.action_range)(median)\n",
    "\n",
    "        self.model = models.Model(inputs=[states], outputs=[output])\n",
    "        \n",
    "        optimizer = optimizers.Adam(lr=self.lr)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "            \n",
    "    def predict(self, state):\n",
    "        prediction = self.model.predict(np.expand_dims(state, axis=0))\n",
    "        \n",
    "        return prediction[0]\n",
    "    \n",
    "    def optimizer(self):\n",
    "        action_gdts = K.placeholder(shape=(None, self.action_size))\n",
    "        params_grad = tf.gradients(self.model.output,\n",
    "                                   self.model.trainable_weights,\n",
    "                                   -action_gdts)\n",
    "        \n",
    "        grads = zip(params_grad, self.model.trainable_weights)\n",
    "        \n",
    "        return K.function([self.model.input, action_gdts],\n",
    "                          [tf.train.AdamOptimizer(self.lr).apply_gradients(grads)])\n",
    "    \n",
    "    def train(self, states, actions, grads):\n",
    "#         self.adam_optimizer([states, grads])\n",
    "        self.model.train_on_batch(states, (actions-grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    def __init__(self, env, lr=0.0001):\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape\n",
    "        self.action_size = env.action_space.shape\n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        self.grads = K.function([self.model.input[0],\n",
    "                                 self.model.input[1]],\n",
    "                                K.gradients(self.model.output,\n",
    "                                            self.model.input[1])) \n",
    "    \n",
    "    def build_model(self):\n",
    "        states = layers.Input(shape=self.state_size, name='states')\n",
    "        actions = layers.Input(shape=self.action_size, name='actions')\n",
    "        \n",
    "        layer_1 = layers.Dense(\n",
    "            units=256,\n",
    "            activation='relu'\n",
    "        )(states)\n",
    "        \n",
    "        layer_2 = layers.concatenate([layer_1, actions])\n",
    "        \n",
    "        layer_3 = layers.Dense(\n",
    "            units=128,\n",
    "            activation='relu'\n",
    "        )(layer_2)\n",
    "    \n",
    "        output = layers.Dense(\n",
    "            units=1,\n",
    "            activation='linear',\n",
    "            kernel_initializer=keras.initializers.RandomUniform()\n",
    "        )(layer_3)\n",
    "\n",
    "        self.model = models.Model(inputs=[states, actions], outputs=output)\n",
    "        \n",
    "        optimizer = optimizers.Adam(lr=self.lr)\n",
    "        \n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        \n",
    "    def action_gradients(self, states, actions):\n",
    "        return self.grads([states, actions])[0]\n",
    "            \n",
    "    def predict(self, state, action):\n",
    "        return self.model.predict([np.expand_dims(state, axis=0),\n",
    "                                   np.expand_dims(action, axis=0)])[0]\n",
    "    \n",
    "    def train_on_batch(self, states, actions, critic_target):\n",
    "        return self.model.train_on_batch([states, actions],\n",
    "                                         critic_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \"\"\" Deep Deterministic Policy Gradient (DDPG) Helper Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, buffer_size = 20000,\n",
    "                 gamma = 0.99, lr = 0.00005, tau = 0.001):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and A2C parameters\n",
    "        \n",
    "        self.state_size = env.observation_space.shape\n",
    "        self.action_size = env.action_space.shape\n",
    "        self.action_high = env.action_space.high\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        # Create actor and critic networks\n",
    "        self.actor = Actor(env, 0.1 * lr)\n",
    "        self.critic = Critic(env, lr)\n",
    "        \n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        self.last_state = state\n",
    "        return self.actor.predict(state)\n",
    "    \n",
    "    def step(self, action, reward, next_state, done):\n",
    "        self.memory.add(self.last_state,\n",
    "                        action,\n",
    "                        reward,\n",
    "                        next_state,\n",
    "                        done)\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def bellman(self, rewards, q_values, dones):\n",
    "        critic_targets = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                critic_targets[i] = rewards[i]\n",
    "            else:\n",
    "                critic_targets[i] = rewards[i] + self.gamma * q_values[i]\n",
    "        return critic_targets\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        self.memory.add(state, action, reward, done, new_state)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        return self.buffer.sample()\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = (np.array([e.action for e in experiences if e is not None])\n",
    "                     .astype(np.float32))\n",
    "        rewards = (np.array([e.reward for e in experiences if e is not None])\n",
    "                     .astype(np.float32))\n",
    "        dones = (np.array([e.done for e in experiences if e is not None])\n",
    "                   .astype(np.uint8))\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        \n",
    "        q_values = self.critic.model.predict([next_states,\n",
    "                                             self.actor.model.predict(next_states)])\n",
    "\n",
    "        targets = self.bellman(rewards, q_values, dones)\n",
    "\n",
    "        self.critic.train_on_batch(states, actions, targets)\n",
    "\n",
    "        actions = self.actor.model.predict(states)\n",
    "        gradients = self.critic.action_gradients(states, actions)\n",
    "\n",
    "        self.actor.train(states, actions, gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'tuple'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1203\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-0bfa9967d592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-157-0226a91a6eec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, buffer_size, gamma, lr, tau)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Create actor and critic networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-d9003bd5228d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, lr)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-d9003bd5228d>\u001b[0m in \u001b[0;36moptimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0maction_gdts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         params_grad = tf.gradients(self.model.output,\n\u001b[1;32m     67\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2141\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   6258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6259\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6260\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6261\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   6262\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'tuple'."
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "agent = DDPG(env)\n",
    "\n",
    "best_score = -np.inf\n",
    "\n",
    "avg_scores = deque(maxlen=num_episodes)\n",
    "\n",
    "score_eval = ScoreEvaluator(1)\n",
    "\n",
    "for i in range(num_episodes):  \n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "#     agent.reset_episode()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "#         agent.step(state, reward, done)\n",
    "        agent.step(action, reward, state, done)\n",
    "        \n",
    "        score += reward\n",
    "        if done:\n",
    "            score_eval.add(score)\n",
    "            print_iteaction(i, score_eval)\n",
    "            break\n",
    "            \n",
    "score_eval.plot_avg_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [GPU]",
   "language": "python",
   "name": "python3-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
